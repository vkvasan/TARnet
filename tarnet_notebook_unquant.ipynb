{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading start...\n",
      "Data loading complete...\n",
      "(4953, 150, 9)\n",
      "(4953,)\n",
      "(1320, 150, 9)\n",
      "(1320,)\n",
      "Data preprocessing start...\n",
      "torch.Size([4992, 150, 9]) torch.Size([4953]) torch.Size([1344, 150, 9]) torch.Size([1320])\n",
      "Data preprocessing complete...\n",
      "Initializing model...\n",
      "Model intialized...\n",
      "Training start...\n",
      "Epoch: 1, TAR Loss: 395.19441759586334 , TASK Loss: 2.1756765650855683\n",
      "Epoch: 2, TAR Loss: 403.4374610185623 , TASK Loss: 1.628631437135277\n",
      "Epoch: 3, TAR Loss: 377.59546560049057 , TASK Loss: 1.3932752512318665\n",
      "Epoch: 4, TAR Loss: 372.9737695455551 , TASK Loss: 1.1498673016823304\n",
      "Epoch: 5, TAR Loss: 370.1213834285736 , TASK Loss: 1.0780935025750062\n",
      "Epoch: 6, TAR Loss: 350.5521152615547 , TASK Loss: 0.9420785294293262\n",
      "Epoch: 7, TAR Loss: 320.25084125995636 , TASK Loss: 0.6593637108966759\n",
      "Epoch: 8, TAR Loss: 307.09757965803146 , TASK Loss: 0.6588748495664027\n",
      "Epoch: 9, TAR Loss: 322.37050372362137 , TASK Loss: 0.8834863935078084\n",
      "Epoch: 10, TAR Loss: 305.71494364738464 , TASK Loss: 0.6445969515133182\n",
      "Epoch: 11, TAR Loss: 274.1534158885479 , TASK Loss: 0.5078364817473369\n",
      "Epoch: 12, TAR Loss: 253.5179416835308 , TASK Loss: 0.5125573264563171\n",
      "Epoch: 13, TAR Loss: 220.38590854406357 , TASK Loss: 0.4491316566317852\n",
      "Epoch: 14, TAR Loss: 198.46071365475655 , TASK Loss: 0.436344764955408\n",
      "Epoch: 15, TAR Loss: 181.25958183407784 , TASK Loss: 0.4097261732640097\n",
      "Epoch: 16, TAR Loss: 166.78130382299423 , TASK Loss: 0.3738820599202144\n",
      "Epoch: 17, TAR Loss: 177.1082153916359 , TASK Loss: 0.5396597628005401\n",
      "Epoch: 18, TAR Loss: 195.8647299706936 , TASK Loss: 0.6000801521081153\n",
      "Epoch: 19, TAR Loss: 151.14451974630356 , TASK Loss: 0.35857011349766454\n",
      "Epoch: 20, TAR Loss: 140.50297477841377 , TASK Loss: 0.35111701826049946\n",
      "Epoch: 21, TAR Loss: 122.37452338635921 , TASK Loss: 0.26038139042138597\n",
      "Epoch: 22, TAR Loss: 119.2359126880765 , TASK Loss: 0.2626375195745348\n",
      "Epoch: 23, TAR Loss: 114.61583412438631 , TASK Loss: 0.26688699764247503\n",
      "Epoch: 24, TAR Loss: 107.5560827627778 , TASK Loss: 0.22762798257941158\n",
      "Epoch: 25, TAR Loss: 147.78325434029102 , TASK Loss: 0.5650333205293266\n",
      "Epoch: 26, TAR Loss: 189.68763455748558 , TASK Loss: 0.5927548216541715\n",
      "Epoch: 27, TAR Loss: 152.8572779893875 , TASK Loss: 0.47963982283312795\n",
      "Epoch: 28, TAR Loss: 123.76163077354431 , TASK Loss: 0.34191175562373965\n",
      "Epoch: 29, TAR Loss: 101.38087359070778 , TASK Loss: 0.25692534644342657\n",
      "Epoch: 30, TAR Loss: 91.23436828702688 , TASK Loss: 0.17933868675386655\n",
      "Epoch: 31, TAR Loss: 84.4460676908493 , TASK Loss: 0.17352322016268412\n",
      "Epoch: 32, TAR Loss: 77.27031967043877 , TASK Loss: 0.13038091629066378\n",
      "Epoch: 33, TAR Loss: 78.6725896447897 , TASK Loss: 0.16971923322885277\n",
      "Epoch: 34, TAR Loss: 85.24271024763584 , TASK Loss: 0.21112806828741068\n",
      "Epoch: 35, TAR Loss: 91.72212078422308 , TASK Loss: 0.24139187258626688\n",
      "Epoch: 36, TAR Loss: 101.21879306435585 , TASK Loss: 0.26987629283026654\n",
      "Epoch: 37, TAR Loss: 97.7207627967 , TASK Loss: 0.2780737241420969\n",
      "Epoch: 38, TAR Loss: 93.10970731079578 , TASK Loss: 0.22571833671188907\n",
      "Epoch: 39, TAR Loss: 87.49797459691763 , TASK Loss: 0.24674772930114539\n",
      "Epoch: 40, TAR Loss: 77.98787782341242 , TASK Loss: 0.17043757199325468\n",
      "Epoch: 41, TAR Loss: 74.40195184201002 , TASK Loss: 0.1317659530878986\n",
      "Epoch: 42, TAR Loss: 68.97804541885853 , TASK Loss: 0.11447077748069925\n",
      "Epoch: 43, TAR Loss: 81.64181987941265 , TASK Loss: 0.22467278118462997\n",
      "Epoch: 44, TAR Loss: 132.32941362261772 , TASK Loss: 0.38853200518990766\n",
      "Epoch: 45, TAR Loss: 171.861173838377 , TASK Loss: 0.7639665086870716\n",
      "Epoch: 46, TAR Loss: 147.32548236846924 , TASK Loss: 0.3884477250053486\n",
      "Epoch: 47, TAR Loss: 96.87439565360546 , TASK Loss: 0.2105029899801464\n",
      "Epoch: 48, TAR Loss: 86.18607690930367 , TASK Loss: 0.17779206131140624\n",
      "Epoch: 49, TAR Loss: 74.70845261961222 , TASK Loss: 0.15973019465043886\n",
      "Epoch: 50, TAR Loss: 69.98734153807163 , TASK Loss: 0.119468373176308\n",
      "Dataset: AF, Acc: 0.9030303030303031\n",
      "Training complete...\n"
     ]
    }
   ],
   "source": [
    "!python3 script.py --dataset AF --task_type classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading start...\n",
      "Data loading complete...\n",
      "(4953, 150, 9)\n",
      "(4953,)\n",
      "(1320, 150, 9)\n",
      "(1320,)\n",
      "Data preprocessing start...\n",
      "torch.Size([4960, 150, 9]) torch.Size([4953]) torch.Size([1328, 150, 9]) torch.Size([1320])\n",
      "Data preprocessing complete...\n",
      "Initializing model...\n",
      "Model intialized...\n",
      "Epoch: 1, TAR Loss: 615.4413601867855 , TASK Loss: 0.6022142564780844\n",
      "Epoch: 2, TAR Loss: 601.5450972244143 , TASK Loss: 0.45676674253362565\n",
      "Epoch: 3, TAR Loss: 507.3820514306426 , TASK Loss: 0.4300757286997622\n",
      "Epoch: 4, TAR Loss: 445.59177579917014 , TASK Loss: 0.3346029596571812\n",
      "Epoch: 5, TAR Loss: 556.7618160881102 , TASK Loss: 0.4802384344002921\n",
      "Epoch: 6, TAR Loss: 639.6364529915154 , TASK Loss: 0.7116628607451861\n",
      "Epoch: 7, TAR Loss: 454.801079954952 , TASK Loss: 0.4050581230884074\n",
      "Epoch: 8, TAR Loss: 463.0538225993514 , TASK Loss: 0.3136676241233911\n",
      "Epoch: 9, TAR Loss: 423.62673129700124 , TASK Loss: 0.28775111857392516\n",
      "Epoch: 10, TAR Loss: 398.0908851567656 , TASK Loss: 0.2648243734245411\n",
      "Epoch: 11, TAR Loss: 491.15017130225897 , TASK Loss: 0.3755752650993432\n",
      "Epoch: 12, TAR Loss: 795.4247653707862 , TASK Loss: 0.7688833777919674\n",
      "Epoch: 13, TAR Loss: 817.3217435032129 , TASK Loss: 0.5645626869015794\n",
      "Epoch: 14, TAR Loss: 588.7408406920731 , TASK Loss: 0.33117341851733223\n",
      "Epoch: 15, TAR Loss: 505.5195708870888 , TASK Loss: 0.2534116038179385\n",
      "Epoch: 16, TAR Loss: 479.3674007579684 , TASK Loss: 0.2971133351742339\n",
      "Epoch: 17, TAR Loss: 441.49271938204765 , TASK Loss: 0.2727111473140314\n",
      "Epoch: 18, TAR Loss: 501.05561504513025 , TASK Loss: 0.37016625627492683\n",
      "Epoch: 19, TAR Loss: 500.98456190899014 , TASK Loss: 0.37927742549990107\n",
      "Epoch: 20, TAR Loss: 433.8782891780138 , TASK Loss: 0.3942208745232062\n",
      "Epoch: 21, TAR Loss: 365.8747798129916 , TASK Loss: 0.22395947330920696\n",
      "Epoch: 22, TAR Loss: 354.379400588572 , TASK Loss: 0.22382518458135942\n",
      "Epoch: 23, TAR Loss: 392.3814310114831 , TASK Loss: 0.280214977111271\n",
      "Epoch: 24, TAR Loss: 467.74815471097827 , TASK Loss: 0.3419127854628215\n",
      "Epoch: 25, TAR Loss: 409.49619260430336 , TASK Loss: 0.27893706393382456\n",
      "Epoch: 26, TAR Loss: 417.3317277766764 , TASK Loss: 0.3603269314435262\n",
      "Epoch: 27, TAR Loss: 373.7237068004906 , TASK Loss: 0.2412493270952434\n",
      "Epoch: 28, TAR Loss: 405.70989634282887 , TASK Loss: 0.24232702231984646\n",
      "Epoch: 29, TAR Loss: 351.9356080945581 , TASK Loss: 0.18383078109800166\n",
      "Epoch: 30, TAR Loss: 434.0857860390097 , TASK Loss: 0.3313953577744689\n",
      "Epoch: 31, TAR Loss: 569.9647706635296 , TASK Loss: 0.4454773531512348\n",
      "Epoch: 32, TAR Loss: 557.1451595984399 , TASK Loss: 0.39737611266171574\n",
      "Epoch: 33, TAR Loss: 413.8992714472115 , TASK Loss: 0.22144021219568275\n",
      "Epoch: 34, TAR Loss: 389.7937385626137 , TASK Loss: 0.22177429354451636\n",
      "Epoch: 35, TAR Loss: 357.61775780282915 , TASK Loss: 0.17374555387964746\n",
      "Epoch: 36, TAR Loss: 337.83785752765834 , TASK Loss: 0.14963984887831402\n",
      "Epoch: 37, TAR Loss: 350.88108159415424 , TASK Loss: 0.2075192475140128\n",
      "Epoch: 38, TAR Loss: 385.561795450747 , TASK Loss: 0.2184427153128644\n",
      "Epoch: 39, TAR Loss: 594.9674673378468 , TASK Loss: 0.437695027096244\n",
      "Epoch: 40, TAR Loss: 504.35121095739305 , TASK Loss: 0.3010704841123198\n",
      "Epoch: 41, TAR Loss: 451.99376402422786 , TASK Loss: 0.20408078981699773\n",
      "Epoch: 42, TAR Loss: 479.7081105783582 , TASK Loss: 0.25224450775977086\n",
      "Epoch: 43, TAR Loss: 431.73368225619197 , TASK Loss: 0.2271901738398483\n",
      "Epoch: 44, TAR Loss: 412.79835673235357 , TASK Loss: 0.2221262112366385\n",
      "Epoch: 45, TAR Loss: 391.75912103429437 , TASK Loss: 0.2124726209597716\n",
      "Epoch: 46, TAR Loss: 401.73475481942296 , TASK Loss: 0.17955591642502783\n",
      "Epoch: 47, TAR Loss: 361.69128671847284 , TASK Loss: 0.19500008879992178\n",
      "Epoch: 48, TAR Loss: 380.8640458807349 , TASK Loss: 0.2032722131609841\n",
      "Epoch: 49, TAR Loss: 441.6693448238075 , TASK Loss: 0.312878138176645\n",
      "Epoch: 50, TAR Loss: 408.82025486789644 , TASK Loss: 0.22086779866348774\n",
      "Epoch: 51, TAR Loss: 407.5703021027148 , TASK Loss: 0.21391550655416672\n",
      "Epoch: 52, TAR Loss: 387.46483954042196 , TASK Loss: 0.24102224010429393\n",
      "Epoch: 53, TAR Loss: 396.9836171194911 , TASK Loss: 0.1700762736827493\n",
      "Epoch: 54, TAR Loss: 357.93024885095656 , TASK Loss: 0.18394199186619564\n",
      "Epoch: 55, TAR Loss: 335.061068572104 , TASK Loss: 0.13297424121836654\n",
      "Epoch: 56, TAR Loss: 337.1237901933491 , TASK Loss: 0.19487886335819915\n",
      "Epoch: 57, TAR Loss: 291.6663365550339 , TASK Loss: 0.1356976093119753\n",
      "Epoch: 58, TAR Loss: 314.3243737500161 , TASK Loss: 0.18577047765114585\n",
      "Epoch: 59, TAR Loss: 422.16863813251257 , TASK Loss: 0.32874043081912474\n",
      "Epoch: 60, TAR Loss: 341.34165613912046 , TASK Loss: 0.17495460851071454\n",
      "Epoch: 61, TAR Loss: 314.057146480307 , TASK Loss: 0.12099135687990842\n",
      "Epoch: 62, TAR Loss: 281.8497341889888 , TASK Loss: 0.13388665545542316\n",
      "Epoch: 63, TAR Loss: 352.016243185848 , TASK Loss: 0.18523158949966467\n",
      "Epoch: 64, TAR Loss: 397.8257829658687 , TASK Loss: 0.3692489664479795\n",
      "Epoch: 65, TAR Loss: 327.4436311610043 , TASK Loss: 0.1990639177809865\n",
      "Epoch: 66, TAR Loss: 276.2023713327944 , TASK Loss: 0.11586291194219231\n",
      "Epoch: 67, TAR Loss: 255.8089790865779 , TASK Loss: 0.0854608455228329\n",
      "Epoch: 68, TAR Loss: 266.20227375254035 , TASK Loss: 0.09917341345303263\n",
      "Epoch: 69, TAR Loss: 363.0735972356051 , TASK Loss: 0.2960673003178635\n",
      "Epoch: 70, TAR Loss: 414.88766960799694 , TASK Loss: 0.21978082060224424\n",
      "Epoch: 71, TAR Loss: 350.57576259970665 , TASK Loss: 0.15543735137932468\n",
      "Epoch: 72, TAR Loss: 297.1869493443519 , TASK Loss: 0.14053943215536738\n",
      "Epoch: 73, TAR Loss: 278.5753503199667 , TASK Loss: 0.09424902336468075\n",
      "Epoch: 74, TAR Loss: 251.05887023359537 , TASK Loss: 0.08684401707751471\n",
      "Epoch: 75, TAR Loss: 375.21237546019256 , TASK Loss: 0.29185683902932846\n",
      "Epoch: 76, TAR Loss: 639.4595167785883 , TASK Loss: 0.5828001885709915\n",
      "Epoch: 77, TAR Loss: 413.84207946807146 , TASK Loss: 0.19551458377955314\n",
      "Epoch: 78, TAR Loss: 392.8299912456423 , TASK Loss: 0.16433270273727763\n",
      "Epoch: 79, TAR Loss: 341.503992466256 , TASK Loss: 0.13725360630321207\n",
      "Epoch: 80, TAR Loss: 336.6011529136449 , TASK Loss: 0.16255323558103998\n",
      "Epoch: 81, TAR Loss: 297.2978841867298 , TASK Loss: 0.12990599814311232\n",
      "Epoch: 82, TAR Loss: 273.1478825788945 , TASK Loss: 0.08277413326851514\n",
      "Epoch: 83, TAR Loss: 320.53503045998514 , TASK Loss: 0.19629823871414945\n",
      "Epoch: 84, TAR Loss: 371.0376528967172 , TASK Loss: 0.24049192960261004\n",
      "Epoch: 85, TAR Loss: 320.2435684874654 , TASK Loss: 0.13730472792102738\n",
      "Epoch: 86, TAR Loss: 299.4813829474151 , TASK Loss: 0.11489029603594922\n",
      "Epoch: 87, TAR Loss: 307.1076492201537 , TASK Loss: 0.12262983497657017\n",
      "Epoch: 88, TAR Loss: 301.6262125521898 , TASK Loss: 0.15373899854616738\n",
      "Epoch: 89, TAR Loss: 305.5438646581024 , TASK Loss: 0.11287345496284915\n",
      "Epoch: 90, TAR Loss: 330.47552938759327 , TASK Loss: 0.13288430283053987\n",
      "Epoch: 91, TAR Loss: 320.42048334144056 , TASK Loss: 0.14553094472968903\n",
      "Epoch: 92, TAR Loss: 305.5422972459346 , TASK Loss: 0.11169628661407112\n",
      "Epoch: 93, TAR Loss: 310.36803370155394 , TASK Loss: 0.15967457432527088\n",
      "Epoch: 94, TAR Loss: 374.30561221949756 , TASK Loss: 0.2872184999129828\n",
      "Epoch: 95, TAR Loss: 336.5899628382176 , TASK Loss: 0.1180132209124585\n",
      "Epoch: 96, TAR Loss: 336.62806130293757 , TASK Loss: 0.13170500069827687\n",
      "Epoch: 97, TAR Loss: 311.989334218204 , TASK Loss: 0.11240565353560182\n",
      "Epoch: 98, TAR Loss: 300.41657853499055 , TASK Loss: 0.10325374494716136\n",
      "Epoch: 99, TAR Loss: 298.17395434342325 , TASK Loss: 0.10788685173939919\n",
      "Epoch: 100, TAR Loss: 311.19065547920763 , TASK Loss: 0.1412641202273927\n",
      "Dataset: AF, Acc: 0.931060606060606\n"
     ]
    }
   ],
   "source": [
    "!python3 script.py --dataset AF --task_type classification --batch 16 --epochs 100 --load True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading complete...\n",
      "(4953, 150, 9)\n",
      "(4953,)\n",
      "(1320, 150, 9)\n",
      "(1320,)\n",
      "Data preprocessing start...\n",
      "torch.Size([4960, 150, 9]) torch.Size([4953]) torch.Size([1328, 150, 9]) torch.Size([1320])\n",
      "Data preprocessing complete...\n",
      "Config(dataset=AF, batch=16, lr=0.001, nlayers=4, emb_size=256, nhead=8, task_rate=0.5, masking_ratio=0.15, lamb=0.8, epochs=1, ratio_highest_attention=0.5, avg=macro, dropout=0.01, nhid=128, nhid_task=128, nhid_tar=128, task_type=classification)\n"
     ]
    }
   ],
   "source": [
    "import multitask_transformer_class\n",
    "import numpy as np\n",
    "import utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.dataset = 'AF'\n",
    "        self.batch = 16\n",
    "        self.lr = 0.001\n",
    "        self.nlayers = 4\n",
    "        self.emb_size = 256\n",
    "        self.nhead = 8\n",
    "        self.task_rate = 0.5\n",
    "        self.masking_ratio = 0.15\n",
    "        self.lamb = 0.8\n",
    "        self.epochs = 1\n",
    "        self.ratio_highest_attention = 0.5\n",
    "        self.avg = 'macro'\n",
    "        self.dropout = 0.01\n",
    "        self.nhid = 128\n",
    "        self.nhid_task = 128\n",
    "        self.nhid_tar = 128\n",
    "        self.task_type = 'classification' #if 'classification' else 0  # Converts to 1 for classification and 0 for regression        \n",
    "        self.device = \"cpu\"\n",
    "        self.nclasses = None\n",
    "        self.seq_len = 0 \n",
    "        self.input_size = 0\n",
    "        #self.nclasses = torch.max(y_train_task).item() + 1 if prop['task_type'] == 'classification' else None\n",
    "        #self.dataset = dataset\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"Config(dataset={self.dataset}, batch={self.batch}, lr={self.lr}, nlayers={self.nlayers}, \"\n",
    "                f\"emb_size={self.emb_size}, nhead={self.nhead}, task_rate={self.task_rate}, \"\n",
    "                f\"masking_ratio={self.masking_ratio}, lamb={self.lamb}, epochs={self.epochs}, \"\n",
    "                f\"ratio_highest_attention={self.ratio_highest_attention}, avg={self.avg}, \"\n",
    "                f\"dropout={self.dropout}, nhid={self.nhid}, nhid_task={self.nhid_task}, \"\n",
    "                f\"nhid_tar={self.nhid_tar}, task_type={self.task_type})\")\n",
    "\n",
    "\n",
    "# Instantiate the class\n",
    "config = Config()\n",
    "\n",
    "X_train = np.load('./easy_imu_phone/x_train.npy')\n",
    "y_train = np.load('./easy_imu_phone/y_train.npy')\n",
    "X_test = np.load('./easy_imu_phone/x_test.npy')\n",
    "y_test = np.load('./easy_imu_phone/y_test.npy')\n",
    "criterion_task = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print('Data loading complete...')\n",
    "print( X_train.shape)\n",
    "print( y_train.shape)\n",
    "print( X_test.shape)\n",
    "print( y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "print('Data preprocessing start...')\n",
    "X_train_task, y_train_task, X_test, y_test = utils.preprocess_nb(config, X_train, y_train, X_test, y_test)\n",
    "print(X_train_task.shape, y_train_task.shape, X_test.shape, y_test.shape)\n",
    "print('Data preprocessing complete...')\n",
    "\n",
    "config.nclasses = torch.max(y_train_task).item() + 1 if config.task_type == 'classification' else None\n",
    "config.seq_len, config.input_size = X_train_task.shape[1], X_train_task.shape[2]\n",
    "#prop['device'] = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "\n",
    "# Access the arguments as attributes\n",
    "print(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import transformer\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        max_len = max(5000, seq_len)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        if d_model % 2 == 0:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)[:, 0:-1]\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Permute(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.permute(1, 0)\n",
    "\n",
    "class MultitaskTransformerModel(nn.Module):\n",
    "    def __init__(self, task_type, device, nclasses, seq_len, batch, input_size, emb_size, nhead, nhid, nhid_tar, nhid_task, nlayers, dropout=0.1):\n",
    "        super(MultitaskTransformerModel, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.trunk_net = nn.Sequential(\n",
    "            nn.Linear(input_size, emb_size).to(device),\n",
    "            nn.BatchNorm1d(batch).to(device),\n",
    "            PositionalEncoding(seq_len, emb_size, dropout).to(device),\n",
    "            nn.BatchNorm1d(batch).to(device)\n",
    "        )\n",
    "        \n",
    "        encoder_layers = transformer.TransformerEncoderLayer(emb_size, nhead, nhid, dropout).to(device)\n",
    "        self.transformer_encoder = transformer.TransformerEncoder(encoder_layers, nlayers, device).to(device)\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(batch).to(device)\n",
    "        \n",
    "        # Task-aware Reconstruction Layers\n",
    "        self.tar_net = nn.Sequential(\n",
    "            nn.Linear(emb_size, nhid_tar).to(device),\n",
    "            nn.BatchNorm1d(batch).to(device),\n",
    "            nn.Linear(nhid_tar, nhid_tar).to(device),\n",
    "            nn.BatchNorm1d(batch).to(device),\n",
    "            nn.Linear(nhid_tar, input_size).to(device),\n",
    "        )\n",
    "\n",
    "        if task_type == 'classification':\n",
    "            # Classification Layers\n",
    "            self.class_net = nn.Sequential(\n",
    "                nn.Linear(emb_size, nhid_task).to(device),\n",
    "                nn.ReLU().to(device),\n",
    "                Permute().to(device),\n",
    "                nn.BatchNorm1d(batch).to(device),\n",
    "                Permute().to(device),\n",
    "                nn.Dropout(p=0.3).to(device),\n",
    "                nn.Linear(nhid_task, nhid_task).to(device),\n",
    "                nn.ReLU().to(device),\n",
    "                Permute().to(device),\n",
    "                nn.BatchNorm1d(batch).to(device),\n",
    "                Permute().to(device),\n",
    "                nn.Dropout(p=0.3).to(device),\n",
    "                nn.Linear(nhid_task, nclasses).to(device)\n",
    "            )\n",
    "        else:\n",
    "            # Regression Layers\n",
    "            self.reg_net = nn.Sequential(\n",
    "                nn.Linear(emb_size, nhid_task).to(device),\n",
    "                nn.ReLU().to(device),\n",
    "                Permute().to(device),\n",
    "                nn.BatchNorm1d(batch).to(device),\n",
    "                Permute().to(device),\n",
    "                nn.Linear(nhid_task, nhid_task).to(device),\n",
    "                nn.ReLU().to(device),\n",
    "                Permute().to(device),\n",
    "                nn.BatchNorm1d(batch).to(device),\n",
    "                Permute().to(device),\n",
    "                nn.Linear(nhid_task, 1).to(device),\n",
    "            )\n",
    "\n",
    "    def forward(self, x, task_type):\n",
    "        #x = x.to(self.device)\n",
    "        #print(f\"Input x is on device: {x.device}\")\n",
    "        \n",
    "        x = self.trunk_net(x.permute(1, 0, 2))\n",
    "        #print(f\"x after trunk_net is on device: {x.device}\")\n",
    "        \n",
    "        x, attn = self.transformer_encoder(x)\n",
    "        #print(f\"x after transformer_encoder is on device: {x.device}\")\n",
    "        #print(f\"attn after transformer_encoder is on device: {attn.device}\")\n",
    "        \n",
    "        x = self.batch_norm(x)\n",
    "        #print(f\"x after batch_norm is on device: {x.device}\")\n",
    "        \n",
    "        if task_type == 'reconstruction':\n",
    "            output = self.tar_net(x).permute(1, 0, 2)\n",
    "        elif task_type == 'classification':\n",
    "            output = self.class_net(x[-1])\n",
    "        elif task_type == 'regression':\n",
    "            output = self.reg_net(x[-1])\n",
    "        \n",
    "        return output, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the entire model and ensure it is on the correct device\n",
    "device = torch.device('cpu')  # or 'cuda:0' if using GPU\n",
    "old_model = torch.load('train_model.pth', map_location=device)\n",
    "state_dict = old_model.state_dict()\n",
    "\n",
    "model =  MultitaskTransformerModel(config.task_type, config.device, config.nclasses, config.seq_len, config.batch, \\\n",
    "    config.input_size, config.emb_size, config.nhead, config.nhid, config.nhid_tar, config.nhid_task, config.nlayers, config.dropout)\n",
    "model.load_state_dict(state_dict)\n",
    "#model.load_state_dict(torch.load('train_model.pth', map_location=device))  # Move the model to the correct device\n",
    "#model.transformer_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: [0.44092267751693726, 0.8696969696969697, 0.892000990481019, 0.8696969696969696, 0.861144922436161]\n",
      "Average Inference Time per Batch: 0.052317133869033264 seconds\n",
      "Average Inference Time per Sample: 0.003269820866814579 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import multitask_transformer_class\n",
    "import torch\n",
    "import math\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def evaluate(y_pred, y, nclasses, criterion, task_type, device, avg):\n",
    "    results = []\n",
    "\n",
    "    if task_type == 'classification':\n",
    "        loss = criterion(y_pred.view(-1, nclasses), torch.as_tensor(y, device=device)).item()\n",
    "        \n",
    "        pred, target = y_pred.cpu().data.numpy(), y.cpu().data.numpy()\n",
    "        pred = np.argmax(pred, axis=1)\n",
    "        acc = accuracy_score(target, pred)\n",
    "        prec = precision_score(target, pred, average=avg)\n",
    "        rec = recall_score(target, pred, average=avg)\n",
    "        f1 = f1_score(target, pred, average=avg)\n",
    "        \n",
    "        results.extend([loss, acc, prec, rec, f1])\n",
    "    else:\n",
    "        y_pred = y_pred.squeeze()\n",
    "        y = torch.as_tensor(y, device=device)\n",
    "        rmse = math.sqrt(((y_pred - y) ** 2).sum().item() / y_pred.shape[0])\n",
    "        mae = torch.abs(y_pred - y).mean().item()\n",
    "        results.extend([rmse, mae])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def test_with_inference_time(model, X, y, batch, nclasses, criterion, task_type, device, avg):\n",
    "    model.eval()  # Turn on the evaluation mode\n",
    "    model.to(device)  # Ensure the entire model is on the correct device\n",
    "\n",
    "    num_batches = math.ceil(X.shape[0] / batch)\n",
    "    output_arr = []\n",
    "    total_inference_time = 0.0  # Initialize the total inference time\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            start = int(i * batch)\n",
    "            end = int((i + 1) * batch)\n",
    "            num_inst = y[start:end].shape[0]\n",
    "\n",
    "            # Ensure X_batch is on the correct device\n",
    "            X_batch = torch.as_tensor(X[start:end], device=device)\n",
    "            \n",
    "            # Start timing before the model makes predictions\n",
    "            start_time = time.time()\n",
    "            out = model(X_batch, task_type)[0]\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Calculate the time taken for this batch and add it to the total\n",
    "            batch_inference_time = end_time - start_time\n",
    "            total_inference_time += batch_inference_time\n",
    "            \n",
    "            output_arr.append(out[:num_inst])\n",
    "\n",
    "    # Calculate average inference time per batch\n",
    "    avg_inference_time_per_batch = total_inference_time / num_batches\n",
    "    \n",
    "    # Calculate average inference time per sample\n",
    "    avg_inference_time_per_sample = total_inference_time / (num_batches * batch)\n",
    "    \n",
    "    # Evaluate the model predictions\n",
    "    results = evaluate(torch.cat(output_arr, 0), y, nclasses, criterion, task_type, device, avg)\n",
    "    \n",
    "    return results, avg_inference_time_per_batch, avg_inference_time_per_sample\n",
    "\n",
    "# Example of calling the function\n",
    "results, avg_inference_time_per_batch, avg_inference_time_per_sample = test_with_inference_time(\n",
    "    model, X_test, y_test, config.batch, config.nclasses, criterion_task, config.task_type, device, config.avg\n",
    ")\n",
    "print(\"Evaluation Results:\", results)\n",
    "print(\"Average Inference Time per Batch:\", avg_inference_time_per_batch, \"seconds\")\n",
    "print(\"Average Inference Time per Sample:\", avg_inference_time_per_sample, \"seconds\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
