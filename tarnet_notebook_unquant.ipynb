{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading start...\n",
      "Data loading complete...\n",
      "(4953, 150, 9)\n",
      "(4953,)\n",
      "(1320, 150, 9)\n",
      "(1320,)\n",
      "Data preprocessing start...\n",
      "torch.Size([4992, 150, 9]) torch.Size([4953]) torch.Size([1344, 150, 9]) torch.Size([1320])\n",
      "Data preprocessing complete...\n",
      "Initializing model...\n",
      "Model intialized...\n",
      "Training start...\n",
      "Epoch: 1, TAR Loss: 420.65444511175156 , TASK Loss: 2.0729252689418565\n",
      "Epoch: 2, TAR Loss: 396.3774303793907 , TASK Loss: 1.617490057617444\n",
      "Epoch: 3, TAR Loss: 390.01010751724243 , TASK Loss: 1.1513750517059043\n",
      "Epoch: 4, TAR Loss: 376.7105288505554 , TASK Loss: 1.0513830277207594\n",
      "Epoch: 5, TAR Loss: 373.9832491874695 , TASK Loss: 0.9141276160903032\n",
      "Epoch: 6, TAR Loss: 352.5508804321289 , TASK Loss: 0.847461566326746\n",
      "Epoch: 7, TAR Loss: 352.2136881351471 , TASK Loss: 0.7618961417778661\n",
      "Epoch: 8, TAR Loss: 335.4876137971878 , TASK Loss: 0.700511418085176\n",
      "Epoch: 9, TAR Loss: 324.14614444971085 , TASK Loss: 0.7727941712452929\n",
      "Epoch: 10, TAR Loss: 323.10590171813965 , TASK Loss: 0.6685362272361297\n",
      "Epoch: 11, TAR Loss: 293.4885484278202 , TASK Loss: 0.6486445977026324\n",
      "Epoch: 12, TAR Loss: 261.77604988217354 , TASK Loss: 0.4715977432629345\n",
      "Epoch: 13, TAR Loss: 240.288334608078 , TASK Loss: 0.5037023728918724\n",
      "Epoch: 14, TAR Loss: 212.66454723477364 , TASK Loss: 0.41191139708047664\n",
      "Epoch: 15, TAR Loss: 200.07836878299713 , TASK Loss: 0.47443090212641764\n",
      "Epoch: 16, TAR Loss: 190.00121393799782 , TASK Loss: 0.4104673192081772\n",
      "Epoch: 17, TAR Loss: 190.99087411165237 , TASK Loss: 0.41133451629441337\n",
      "Epoch: 18, TAR Loss: 172.5716013610363 , TASK Loss: 0.4307072917109458\n",
      "Epoch: 19, TAR Loss: 143.76044653356075 , TASK Loss: 0.2877125480268587\n",
      "Epoch: 20, TAR Loss: 128.26923793554306 , TASK Loss: 0.2182419040489396\n",
      "Epoch: 21, TAR Loss: 126.33872927725315 , TASK Loss: 0.2476967124581674\n",
      "Epoch: 22, TAR Loss: 132.76759028434753 , TASK Loss: 0.30032412280897747\n",
      "Epoch: 23, TAR Loss: 170.26712614297867 , TASK Loss: 0.3856238802707294\n",
      "Epoch: 24, TAR Loss: 207.32456728816032 , TASK Loss: 0.7139051576541177\n",
      "Epoch: 25, TAR Loss: 173.98635153472424 , TASK Loss: 0.47200105900244627\n",
      "Epoch: 26, TAR Loss: 135.10180261731148 , TASK Loss: 0.3509423634319047\n",
      "Epoch: 27, TAR Loss: 107.83249118924141 , TASK Loss: 0.22550710031684498\n",
      "Epoch: 28, TAR Loss: 111.44982066750526 , TASK Loss: 0.308518939822658\n",
      "Epoch: 29, TAR Loss: 99.6547067090869 , TASK Loss: 0.22102845619448988\n",
      "Epoch: 30, TAR Loss: 97.47431926429272 , TASK Loss: 0.20626693863383885\n",
      "Epoch: 31, TAR Loss: 96.16792847961187 , TASK Loss: 0.22076912731151857\n",
      "Epoch: 32, TAR Loss: 89.27468383312225 , TASK Loss: 0.20779838632221934\n",
      "Epoch: 33, TAR Loss: 84.99853701889515 , TASK Loss: 0.15906238357771818\n",
      "Epoch: 34, TAR Loss: 79.73724436759949 , TASK Loss: 0.11816713401346206\n",
      "Epoch: 35, TAR Loss: 75.75450202822685 , TASK Loss: 0.10753906825596837\n",
      "Epoch: 36, TAR Loss: 69.58983524516225 , TASK Loss: 0.08866666612476376\n",
      "Epoch: 37, TAR Loss: 66.73373195528984 , TASK Loss: 0.06516650792700365\n",
      "Epoch: 38, TAR Loss: 66.95628491789103 , TASK Loss: 0.08699547928469713\n",
      "Epoch: 39, TAR Loss: 74.74283373355865 , TASK Loss: 0.11589428787497631\n",
      "Epoch: 40, TAR Loss: 109.57825387269258 , TASK Loss: 0.42474600332675366\n",
      "Epoch: 41, TAR Loss: 229.70712515711784 , TASK Loss: 0.9469434218095717\n",
      "Epoch: 42, TAR Loss: 215.61524721980095 , TASK Loss: 0.5425923406922099\n",
      "Epoch: 43, TAR Loss: 127.22544506192207 , TASK Loss: 0.2992747806825117\n",
      "Epoch: 44, TAR Loss: 101.4331406801939 , TASK Loss: 0.20265761793779302\n",
      "Epoch: 45, TAR Loss: 87.09831856936216 , TASK Loss: 0.1751849563981696\n",
      "Epoch: 46, TAR Loss: 77.62606482952833 , TASK Loss: 0.1490444991929961\n",
      "Epoch: 47, TAR Loss: 77.10958489775658 , TASK Loss: 0.17433359526597375\n",
      "Epoch: 48, TAR Loss: 82.87960987538099 , TASK Loss: 0.18358274932718327\n",
      "Epoch: 49, TAR Loss: 92.63921894133091 , TASK Loss: 0.21493240691210555\n",
      "Epoch: 50, TAR Loss: 88.42288013547659 , TASK Loss: 0.1940785913701665\n",
      "Epoch: 51, TAR Loss: 79.27259035408497 , TASK Loss: 0.14219061904418562\n",
      "Epoch: 52, TAR Loss: 67.5372681170702 , TASK Loss: 0.10776836511645337\n",
      "Epoch: 53, TAR Loss: 68.70079270377755 , TASK Loss: 0.10681612975412398\n",
      "Epoch: 54, TAR Loss: 68.47584132105112 , TASK Loss: 0.10551729080979516\n",
      "Epoch: 55, TAR Loss: 67.66299387812614 , TASK Loss: 0.09877627079891989\n",
      "Epoch: 56, TAR Loss: 71.3298052214086 , TASK Loss: 0.0929243054405282\n",
      "Epoch: 57, TAR Loss: 65.74887415766716 , TASK Loss: 0.10065955579269195\n",
      "Epoch: 58, TAR Loss: 73.03353705257177 , TASK Loss: 0.15053408808187851\n",
      "Epoch: 59, TAR Loss: 96.86349552124739 , TASK Loss: 0.39417706281690396\n",
      "Epoch: 60, TAR Loss: 106.29292692244053 , TASK Loss: 0.39994921608188794\n",
      "Epoch: 61, TAR Loss: 81.44818382710218 , TASK Loss: 0.16546674403667058\n",
      "Epoch: 62, TAR Loss: 72.8476049900055 , TASK Loss: 0.14238863615144967\n",
      "Epoch: 63, TAR Loss: 66.59516400843859 , TASK Loss: 0.12431457739186068\n",
      "Epoch: 64, TAR Loss: 70.24659392237663 , TASK Loss: 0.13654231036015818\n",
      "Epoch: 65, TAR Loss: 65.67169117927551 , TASK Loss: 0.1381931634657885\n",
      "Epoch: 66, TAR Loss: 62.5901636146009 , TASK Loss: 0.12247226649819674\n",
      "Epoch: 67, TAR Loss: 57.689835362136364 , TASK Loss: 0.08113394969602195\n",
      "Epoch: 68, TAR Loss: 56.90595128759742 , TASK Loss: 0.06847844362134246\n",
      "Epoch: 69, TAR Loss: 54.20973877608776 , TASK Loss: 0.04423753209492952\n",
      "Epoch: 70, TAR Loss: 50.83248395845294 , TASK Loss: 0.03896596432607349\n",
      "Epoch: 71, TAR Loss: 51.65134475007653 , TASK Loss: 0.029715872605945354\n",
      "Epoch: 72, TAR Loss: 49.051470294594765 , TASK Loss: 0.029744241038457832\n",
      "Epoch: 73, TAR Loss: 46.98178654536605 , TASK Loss: 0.03707906221441841\n",
      "Epoch: 74, TAR Loss: 45.87888743914664 , TASK Loss: 0.029829973025973636\n",
      "Epoch: 75, TAR Loss: 51.54618167877197 , TASK Loss: 0.11730442954738869\n",
      "Epoch: 76, TAR Loss: 77.49342595040798 , TASK Loss: 0.2924466967923681\n",
      "Epoch: 77, TAR Loss: 166.39869648218155 , TASK Loss: 0.6381459385365347\n",
      "Epoch: 78, TAR Loss: 230.806422829628 , TASK Loss: 0.9276847917903963\n",
      "Epoch: 79, TAR Loss: 184.7525411248207 , TASK Loss: 0.44036212984579504\n",
      "Epoch: 80, TAR Loss: 132.76242625713348 , TASK Loss: 0.23405987714638593\n",
      "Epoch: 81, TAR Loss: 109.02980701625347 , TASK Loss: 0.17018461526277429\n",
      "Epoch: 82, TAR Loss: 93.31801180541515 , TASK Loss: 0.12008303580705532\n",
      "Epoch: 83, TAR Loss: 83.92367309331894 , TASK Loss: 0.09213172291205621\n",
      "Epoch: 84, TAR Loss: 77.33125615119934 , TASK Loss: 0.1007057012715508\n",
      "Epoch: 85, TAR Loss: 72.97170708328485 , TASK Loss: 0.09652043167396766\n",
      "Epoch: 86, TAR Loss: 68.1116520613432 , TASK Loss: 0.0682526787238177\n",
      "Epoch: 87, TAR Loss: 65.4459587931633 , TASK Loss: 0.06103870104862497\n",
      "Epoch: 88, TAR Loss: 62.02251448854804 , TASK Loss: 0.051071618366655344\n",
      "Epoch: 89, TAR Loss: 59.11782859638333 , TASK Loss: 0.04827551062188478\n",
      "Epoch: 90, TAR Loss: 57.257293693721294 , TASK Loss: 0.05202073695606367\n",
      "Epoch: 91, TAR Loss: 57.402601677924395 , TASK Loss: 0.07110775657315144\n",
      "Epoch: 92, TAR Loss: 73.86570239439607 , TASK Loss: 0.22114832433510903\n",
      "Epoch: 93, TAR Loss: 106.51459188759327 , TASK Loss: 0.5057551989367638\n",
      "Epoch: 94, TAR Loss: 113.31641171872616 , TASK Loss: 0.4004079190362616\n",
      "Epoch: 95, TAR Loss: 90.55428472161293 , TASK Loss: 0.18073057805007678\n",
      "Epoch: 96, TAR Loss: 69.01370575278997 , TASK Loss: 0.11306992766910676\n",
      "Epoch: 97, TAR Loss: 61.940958477556705 , TASK Loss: 0.06868130334124543\n",
      "Epoch: 98, TAR Loss: 60.95906575396657 , TASK Loss: 0.0505978246318173\n",
      "Epoch: 99, TAR Loss: 57.796203058212996 , TASK Loss: 0.0574852572507351\n",
      "Epoch: 100, TAR Loss: 55.2009755782783 , TASK Loss: 0.056326247835904134\n",
      "Dataset: AF, Acc: 0.9401515151515152\n",
      "Training complete...\n"
     ]
    }
   ],
   "source": [
    "!python3 script.py --dataset AF --task_type classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading complete...\n",
      "(4953, 150, 9)\n",
      "(4953,)\n",
      "(1320, 150, 9)\n",
      "(1320,)\n",
      "Data preprocessing start...\n",
      "torch.Size([4992, 150, 9]) torch.Size([4953]) torch.Size([1344, 150, 9]) torch.Size([1320])\n",
      "Data preprocessing complete...\n",
      "Config(dataset=AF, batch=64, lr=0.001, nlayers=4, emb_size=256, nhead=8, task_rate=0.5, masking_ratio=0.15, lamb=0.8, epochs=50, ratio_highest_attention=0.5, avg=macro, dropout=0.01, nhid=128, nhid_task=128, nhid_tar=128, task_type=classification)\n"
     ]
    }
   ],
   "source": [
    "import multitask_transformer_class\n",
    "import numpy as np\n",
    "import utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.dataset = 'AF'\n",
    "        self.batch = 64\n",
    "        self.lr = 0.001\n",
    "        self.nlayers = 4\n",
    "        self.emb_size = 256\n",
    "        self.nhead = 8\n",
    "        self.task_rate = 0.5\n",
    "        self.masking_ratio = 0.15\n",
    "        self.lamb = 0.8\n",
    "        self.epochs = 50\n",
    "        self.ratio_highest_attention = 0.5\n",
    "        self.avg = 'macro'\n",
    "        self.dropout = 0.01\n",
    "        self.nhid = 128\n",
    "        self.nhid_task = 128\n",
    "        self.nhid_tar = 128\n",
    "        self.task_type = 'classification' #if 'classification' else 0  # Converts to 1 for classification and 0 for regression        \n",
    "        self.device = \"cpu\"\n",
    "        self.nclasses = None\n",
    "        self.seq_len = 0 \n",
    "        self.input_size = 0\n",
    "        #self.nclasses = torch.max(y_train_task).item() + 1 if prop['task_type'] == 'classification' else None\n",
    "        #self.dataset = dataset\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"Config(dataset={self.dataset}, batch={self.batch}, lr={self.lr}, nlayers={self.nlayers}, \"\n",
    "                f\"emb_size={self.emb_size}, nhead={self.nhead}, task_rate={self.task_rate}, \"\n",
    "                f\"masking_ratio={self.masking_ratio}, lamb={self.lamb}, epochs={self.epochs}, \"\n",
    "                f\"ratio_highest_attention={self.ratio_highest_attention}, avg={self.avg}, \"\n",
    "                f\"dropout={self.dropout}, nhid={self.nhid}, nhid_task={self.nhid_task}, \"\n",
    "                f\"nhid_tar={self.nhid_tar}, task_type={self.task_type})\")\n",
    "\n",
    "\n",
    "# Instantiate the class\n",
    "config = Config()\n",
    "\n",
    "X_train = np.load('./easy_imu_phone/x_train.npy')\n",
    "y_train = np.load('./easy_imu_phone/y_train.npy')\n",
    "X_test = np.load('./easy_imu_phone/x_test.npy')\n",
    "y_test = np.load('./easy_imu_phone/y_test.npy')\n",
    "criterion_task = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print('Data loading complete...')\n",
    "print( X_train.shape)\n",
    "print( y_train.shape)\n",
    "print( X_test.shape)\n",
    "print( y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "print('Data preprocessing start...')\n",
    "X_train_task, y_train_task, X_test, y_test = utils.preprocess_nb(config, X_train, y_train, X_test, y_test)\n",
    "print(X_train_task.shape, y_train_task.shape, X_test.shape, y_test.shape)\n",
    "print('Data preprocessing complete...')\n",
    "\n",
    "config.nclasses = torch.max(y_train_task).item() + 1 if config.task_type == 'classification' else None\n",
    "config.seq_len, config.input_size = X_train_task.shape[1], X_train_task.shape[2]\n",
    "#prop['device'] = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "\n",
    "# Access the arguments as attributes\n",
    "print(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import transformer\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        max_len = max(5000, seq_len)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        if d_model % 2 == 0:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)[:, 0:-1]\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Permute(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.permute(1, 0)\n",
    "\n",
    "class MultitaskTransformerModel(nn.Module):\n",
    "    def __init__(self, task_type, device, nclasses, seq_len, batch, input_size, emb_size, nhead, nhid, nhid_tar, nhid_task, nlayers, dropout=0.1):\n",
    "        super(MultitaskTransformerModel, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.trunk_net = nn.Sequential(\n",
    "            nn.Linear(input_size, emb_size).to(device),\n",
    "            nn.BatchNorm1d(batch).to(device),\n",
    "            PositionalEncoding(seq_len, emb_size, dropout).to(device),\n",
    "            nn.BatchNorm1d(batch).to(device)\n",
    "        )\n",
    "        \n",
    "        encoder_layers = transformer.TransformerEncoderLayer(emb_size, nhead, nhid, dropout).to(device)\n",
    "        self.transformer_encoder = transformer.TransformerEncoder(encoder_layers, nlayers, device).to(device)\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(batch).to(device)\n",
    "        \n",
    "        # Task-aware Reconstruction Layers\n",
    "        self.tar_net = nn.Sequential(\n",
    "            nn.Linear(emb_size, nhid_tar).to(device),\n",
    "            nn.BatchNorm1d(batch).to(device),\n",
    "            nn.Linear(nhid_tar, nhid_tar).to(device),\n",
    "            nn.BatchNorm1d(batch).to(device),\n",
    "            nn.Linear(nhid_tar, input_size).to(device),\n",
    "        )\n",
    "\n",
    "        if task_type == 'classification':\n",
    "            # Classification Layers\n",
    "            self.class_net = nn.Sequential(\n",
    "                nn.Linear(emb_size, nhid_task).to(device),\n",
    "                nn.ReLU().to(device),\n",
    "                Permute().to(device),\n",
    "                nn.BatchNorm1d(batch).to(device),\n",
    "                Permute().to(device),\n",
    "                nn.Dropout(p=0.3).to(device),\n",
    "                nn.Linear(nhid_task, nhid_task).to(device),\n",
    "                nn.ReLU().to(device),\n",
    "                Permute().to(device),\n",
    "                nn.BatchNorm1d(batch).to(device),\n",
    "                Permute().to(device),\n",
    "                nn.Dropout(p=0.3).to(device),\n",
    "                nn.Linear(nhid_task, nclasses).to(device)\n",
    "            )\n",
    "        else:\n",
    "            # Regression Layers\n",
    "            self.reg_net = nn.Sequential(\n",
    "                nn.Linear(emb_size, nhid_task).to(device),\n",
    "                nn.ReLU().to(device),\n",
    "                Permute().to(device),\n",
    "                nn.BatchNorm1d(batch).to(device),\n",
    "                Permute().to(device),\n",
    "                nn.Linear(nhid_task, nhid_task).to(device),\n",
    "                nn.ReLU().to(device),\n",
    "                Permute().to(device),\n",
    "                nn.BatchNorm1d(batch).to(device),\n",
    "                Permute().to(device),\n",
    "                nn.Linear(nhid_task, 1).to(device),\n",
    "            )\n",
    "\n",
    "    def forward(self, x, task_type):\n",
    "        #x = x.to(self.device)\n",
    "        #print(f\"Input x is on device: {x.device}\")\n",
    "        \n",
    "        x = self.trunk_net(x.permute(1, 0, 2))\n",
    "        #print(f\"x after trunk_net is on device: {x.device}\")\n",
    "        \n",
    "        x, attn = self.transformer_encoder(x)\n",
    "        #print(f\"x after transformer_encoder is on device: {x.device}\")\n",
    "        #print(f\"attn after transformer_encoder is on device: {attn.device}\")\n",
    "        \n",
    "        x = self.batch_norm(x)\n",
    "        #print(f\"x after batch_norm is on device: {x.device}\")\n",
    "        \n",
    "        if task_type == 'reconstruction':\n",
    "            output = self.tar_net(x).permute(1, 0, 2)\n",
    "        elif task_type == 'classification':\n",
    "            output = self.class_net(x[-1])\n",
    "        elif task_type == 'regression':\n",
    "            output = self.reg_net(x[-1])\n",
    "        \n",
    "        return output, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the entire model and ensure it is on the correct device\n",
    "device = torch.device('cpu')  # or 'cuda:0' if using GPU\n",
    "old_model = torch.load('train_model.pth', map_location=device)\n",
    "state_dict = old_model.state_dict()\n",
    "\n",
    "model =  MultitaskTransformerModel(config.task_type, config.device, config.nclasses, config.seq_len, config.batch, \\\n",
    "    config.input_size, config.emb_size, config.nhead, config.nhid, config.nhid_tar, config.nhid_task, config.nlayers, config.dropout)\n",
    "model.load_state_dict(state_dict)\n",
    "#model.load_state_dict(torch.load('train_model.pth', map_location=device))  # Move the model to the correct device\n",
    "#model.transformer_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: [0.3524051308631897, 0.9159090909090909, 0.9261410690619131, 0.915909090909091, 0.909828509524417]\n",
      "Average Inference Time per Batch: 0.18899252301170713 seconds\n",
      "Average Inference Time per Sample: 0.002953008172057924 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import multitask_transformer_class\n",
    "import torch\n",
    "import math\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def evaluate(y_pred, y, nclasses, criterion, task_type, device, avg):\n",
    "    results = []\n",
    "\n",
    "    if task_type == 'classification':\n",
    "        loss = criterion(y_pred.view(-1, nclasses), torch.as_tensor(y, device=device)).item()\n",
    "        \n",
    "        pred, target = y_pred.cpu().data.numpy(), y.cpu().data.numpy()\n",
    "        pred = np.argmax(pred, axis=1)\n",
    "        acc = accuracy_score(target, pred)\n",
    "        prec = precision_score(target, pred, average=avg)\n",
    "        rec = recall_score(target, pred, average=avg)\n",
    "        f1 = f1_score(target, pred, average=avg)\n",
    "        \n",
    "        results.extend([loss, acc, prec, rec, f1])\n",
    "    else:\n",
    "        y_pred = y_pred.squeeze()\n",
    "        y = torch.as_tensor(y, device=device)\n",
    "        rmse = math.sqrt(((y_pred - y) ** 2).sum().item() / y_pred.shape[0])\n",
    "        mae = torch.abs(y_pred - y).mean().item()\n",
    "        results.extend([rmse, mae])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def test_with_inference_time(model, X, y, batch, nclasses, criterion, task_type, device, avg):\n",
    "    model.eval()  # Turn on the evaluation mode\n",
    "    model.to(device)  # Ensure the entire model is on the correct device\n",
    "\n",
    "    num_batches = math.ceil(X.shape[0] / batch)\n",
    "    output_arr = []\n",
    "    total_inference_time = 0.0  # Initialize the total inference time\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            start = int(i * batch)\n",
    "            end = int((i + 1) * batch)\n",
    "            num_inst = y[start:end].shape[0]\n",
    "\n",
    "            # Ensure X_batch is on the correct device\n",
    "            X_batch = torch.as_tensor(X[start:end], device=device)\n",
    "            \n",
    "            # Start timing before the model makes predictions\n",
    "            start_time = time.time()\n",
    "            out = model(X_batch, task_type)[0]\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Calculate the time taken for this batch and add it to the total\n",
    "            batch_inference_time = end_time - start_time\n",
    "            total_inference_time += batch_inference_time\n",
    "            \n",
    "            output_arr.append(out[:num_inst])\n",
    "\n",
    "    # Calculate average inference time per batch\n",
    "    avg_inference_time_per_batch = total_inference_time / num_batches\n",
    "    \n",
    "    # Calculate average inference time per sample\n",
    "    avg_inference_time_per_sample = total_inference_time / (num_batches * batch)\n",
    "    \n",
    "    # Evaluate the model predictions\n",
    "    results = evaluate(torch.cat(output_arr, 0), y, nclasses, criterion, task_type, device, avg)\n",
    "    \n",
    "    return results, avg_inference_time_per_batch, avg_inference_time_per_sample\n",
    "\n",
    "# Example of calling the function\n",
    "results, avg_inference_time_per_batch, avg_inference_time_per_sample = test_with_inference_time(\n",
    "    model, X_test, y_test, config.batch, config.nclasses, criterion_task, config.task_type, device, config.avg\n",
    ")\n",
    "print(\"Evaluation Results:\", results)\n",
    "print(\"Average Inference Time per Batch:\", avg_inference_time_per_batch, \"seconds\")\n",
    "print(\"Average Inference Time per Sample:\", avg_inference_time_per_sample, \"seconds\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
